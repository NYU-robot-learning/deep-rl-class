<!doctype html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-alpha.64">
<link rel="search" type="application/opensearchdescription+xml" title="Deep Reinforcement Learning" href="/opensearch.xml"><title data-react-helmet="true">Course Schedule | Deep Reinforcement Learning</title><meta data-react-helmet="true" name="docsearch:version" content="current,latest"><meta data-react-helmet="true" property="og:image" content="https://create-react-app.dev/img/logo-og.png"><meta data-react-helmet="true" property="twitter:image" content="https://create-react-app.dev/img/logo-og.png"><meta data-react-helmet="true" name="twitter:image:alt" content="Image for Deep Reinforcement Learning"><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:title" content="Course Schedule | Deep Reinforcement Learning"><meta data-react-helmet="true" name="description" content="| wk | Lecture | Slides | Reading material |"><meta data-react-helmet="true" property="og:description" content="| wk | Lecture | Slides | Reading material |"><meta data-react-helmet="true" property="og:url" content="https://create-react-app.dev/docs/lectures"><link data-react-helmet="true" rel="shortcut icon" href="/img/favicon/favicon.ico"><link data-react-helmet="true" rel="preconnect" href="https://BH4D9OD16A-dsn.algolia.net" crossorigin="true"><link data-react-helmet="true" rel="canonical" href="https://create-react-app.dev/docs/lectures"><link rel="stylesheet" href="/styles.5a11437a.css">
<link rel="preload" href="/styles.bbcfc533.js" as="script">
<link rel="preload" href="/runtime~main.f419612e.js" as="script">
<link rel="preload" href="/main.99f6f5e6.js" as="script">
<link rel="preload" href="/common.13e9cc2e.js" as="script">
<link rel="preload" href="/13.3d134228.js" as="script">
<link rel="preload" href="/16.0e408970.js" as="script">
<link rel="preload" href="/935f2afb.3e930e98.js" as="script">
<link rel="preload" href="/17896441.a64c684f.js" as="script">
<link rel="preload" href="/883ca456.aa2bee24.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><div aria-label="Navigation bar toggle" class="navbar__toggle" role="button" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" width="30" height="30" viewBox="0 0 30 30" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></div><a class="navbar__brand" href="/"><strong class="navbar__title">Deep Reinforcement Learning</strong></a><a class="navbar__item navbar__link" href="/docs/logistics">Logistics</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/lectures">Schedule</a><a class="navbar__item navbar__link" href="/docs/assignments">Assignments</a></div><div class="navbar__items navbar__items--right"><div class="react-toggle react-toggle--disabled displayOnlyInLargeViewport_2aTZ"><div class="react-toggle-track"><div class="react-toggle-track-check"><span class="toggle_BsTx">ðŸŒœ</span></div><div class="react-toggle-track-x"><span class="toggle_BsTx">ðŸŒž</span></div></div><div class="react-toggle-thumb"></div><input type="checkbox" disabled="" aria-label="Dark mode toggle" class="react-toggle-screenreader-only"></div><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span><span class="DocSearch-Button-Key">âŒ˜</span><span class="DocSearch-Button-Key">K</span></button></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div><div class="navbar-sidebar"><div class="navbar-sidebar__brand"><a class="navbar__brand" href="/"><strong class="navbar__title">Deep Reinforcement Learning</strong></a></div><div class="navbar-sidebar__items"><div class="menu"><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" href="/docs/logistics">Logistics</a></li><li class="menu__list-item"><a aria-current="page" class="menu__link navbar__link--active" href="/docs/lectures">Schedule</a></li><li class="menu__list-item"><a class="menu__link" href="/docs/assignments">Assignments</a></li></ul></div></div></div></nav><div class="main-wrapper"><div class="docPage_2gpo"><main class="docMainContainer_3EyW"><div class="container padding-vert--lg docItemWrapper_1EkI"><div class="row"><div class="col docItemCol_2ASc"><div class="docItemContainer_3QWW"><article><header><h1 class="docTitle_1Lrw">Course Schedule</h1></header><div class="markdown"><table><thead><tr><th>wk</th><th>Lecture</th><th>Slides</th><th>Reading material</th></tr></thead><tbody><tr><td>9/1</td><td>HW-0 Release</td><td></td><td>Fundamentals of Linear Algebra and Deep Learning</td></tr><tr><td>9/3</td><td>Part 1: Course overview <br>Part 2: Supervised learning for control</td><td></td><td><br>1. <a href="http://www.cse.unsw.edu.au/~claude/papers/MI15.pdf" target="_blank" rel="noopener noreferrer">A framework for behavioural cloning, Bain and Sommut, 1999.</a><br>2. <a href="http://proceedings.mlr.press/v15/ross11a/ross11a.pdf" target="_blank" rel="noopener noreferrer">A reduction of imitation learning and structured prediction to no-regret online learning, Ross et al., 2011.</a></td></tr><tr><td>9/10</td><td>Part 1: Introduction to RL<br>Part 2: Value functions</td><td></td><td>Part 1: Chapter 1 of <a href="http://incompleteideas.net/book/RLbook2020.pdf" target="_blank" rel="noopener noreferrer">RL book</a>.<br>Part 2: Chapter 3 of <a href="http://incompleteideas.net/book/RLbook2020.pdf" target="_blank" rel="noopener noreferrer">RL book</a>.</td></tr><tr><td>9/10</td><td>HW-0 Due</td><td></td><td></td></tr><tr><td>9/10</td><td>HW-1 Release</td><td></td><td>Imitation via Supervision.</td></tr><tr><td>9/17</td><td>Deep Q Learning</td><td></td><td>1. <a href="https://daiwk.github.io/assets/dqn.pdf" target="_blank" rel="noopener noreferrer">Human-level control through deep reinforcement learning, Mnih et al., 2015</a>.<br>2. <a href="https://arxiv.org/pdf/1710.02298.pdf" target="_blank" rel="noopener noreferrer">Rainbow: Combining Improvements in Deep Reinforcement Learning, Hessel et al., 2017.</a>Â <br> 3. <a href="https://deepmind.com/blog/article/Agent57-Outperforming-the-human-Atari-benchmark" target="_blank" rel="noopener noreferrer">Agent 57, Deepmind blog post</a></td></tr><tr><td>9/17</td><td>HW-1 Due</td><td></td><td></td></tr><tr><td>9/17</td><td>HW-2 Release</td><td></td><td>Deep Q Learning++.</td></tr><tr><td>9/24</td><td>Policy gradients</td><td></td><td>1. <a href="https://link.springer.com/content/pdf/10.1007/BF00992696.pdf" target="_blank" rel="noopener noreferrer">REINFORCE, Williams, 1992.</a><br>2. <a href="https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf" target="_blank" rel="noopener noreferrer">A Natural Policy Gradient, Kakade, 2002</a>.<br>3. <a href="https://arxiv.org/pdf/1707.06347.pdf" target="_blank" rel="noopener noreferrer">Proximal Policy Optimization Algorithms, Schulman et al. 2017.</a></td></tr><tr><td>10/1</td><td>Part 1: Actor-critic methods<br>Part 2: Distributed RL</td><td></td><td>1. <a href="http://proceedings.mlr.press/v32/silver14.pdf" target="_blank" rel="noopener noreferrer">Deterministic Policy Gradient Algorithms, Silver et al., 2014.</a><br>2. <a href="https://arxiv.org/pdf/1602.01783.pdf" target="_blank" rel="noopener noreferrer">Asynchronous Methods for Deep Reinforcement Learning, Mnih et al. 2016.</a><br>3. <a href="https://arxiv.org/pdf/1507.04296.pdf" target="_blank" rel="noopener noreferrer">Massively Parallel Methods for Deep Reinforcement Learning, Nair et al. 2015.</a></td></tr><tr><td>10/1</td><td>HW-2 Due</td><td></td><td></td></tr><tr><td>10/1</td><td>HW-3 Release</td><td></td><td>RL with Policy Gradients.</td></tr><tr><td>10/8</td><td>Exploration in RL</td><td></td><td>1. <a href="https://lilianweng.github.io/lil-log/2020/06/07/exploration-strategies-in-deep-reinforcement-learning.html" target="_blank" rel="noopener noreferrer">Exploration strategies in deep RL, blog by Lilian Weng, 2020</a> <br> 2. <a href="http://www.pyoudeyer.com/ims.pdf" target="_blank" rel="noopener noreferrer">Intrinsic Motivation Systems for Autonomous Mental Development, Oudeyer et al. 2007.</a> <br> 3. <a href="https://arxiv.org/pdf/1705.05363.pdf" target="_blank" rel="noopener noreferrer">Curiosity-driven Exploration by Self-supervised Prediction, Pathak et al. 2017.</a></td></tr><tr><td>10/9</td><td>Project Proposals Due</td><td></td><td></td></tr><tr><td>10/15</td><td>Generalization in RL</td><td></td><td>1. <a href="https://openai.com/blog/quantifying-generalization-in-reinforcement-learning" target="_blank" rel="noopener noreferrer">Quantifying Generalization in RL, blog by OpenAI 2018</a><br> 2. <a href="https://arxiv.org/pdf/1509.06825.pdf" target="_blank" rel="noopener noreferrer">Supersizing Self-supervision: Learning to Grasp from 50K Tries and 700 Robot Hours, Pinto and Gupta 2016</a> <br>3. <a href="https://arxiv.org/pdf/2008.04899.pdf" target="_blank" rel="noopener noreferrer">Visual Imitation Made Easy, Young et al. 2020</a></td></tr><tr><td>10/15</td><td>HW-3 Due</td><td></td><td></td></tr><tr><td>10/15</td><td>HW-4 Release</td><td></td><td>Exploration with Bandits.</td></tr><tr><td>10/22</td><td>Imitation Learning</td><td></td><td>1. <a href="https://ai.stanford.edu/~ang/papers/icml04-apprentice.pdf" target="_blank" rel="noopener noreferrer">Apprenticeship Learning via Inverse Reinforcement Learning, Abbeel and Ng 2004</a> <br>2. <a href="https://arxiv.org/pdf/1606.03476.pdf" target="_blank" rel="noopener noreferrer">Generative Adversarial Imitation Learning, Ho et al. 2016</a> <br>3. <a href="https://arxiv.org/pdf/1709.10087.pdf" target="_blank" rel="noopener noreferrer">Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations, Rajeswaran et al. 2018</a></td></tr><tr><td>10/22</td><td>HW-4 Due</td><td></td><td></td></tr><tr><td>10/22</td><td>HW-5 Release</td><td></td><td>Adaptive Control and Optimization.</td></tr><tr><td>10/29</td><td>Control and planning</td><td></td><td>1. <a href="https://homes.cs.washington.edu/~todorov/papers/LiICINCO04.pdf" target="_blank" rel="noopener noreferrer">Iterative Linear Quadratic Regulator Design for Nonlinear Biological Movement Systems, Li and Todorov 2004</a> <br>2. <a href="https://arxiv.org/pdf/1907.02057.pdf" target="_blank" rel="noopener noreferrer">Benchmarking Model-Based Reinforcement Learning, Wang et al. 2019</a></td></tr><tr><td>10/29</td><td>HW-5 Due</td><td></td><td></td></tr><tr><td>10/29</td><td>HW-6 Release</td><td></td><td>Learning General Policies.</td></tr><tr><td>11/5</td><td>Part 1: Meta learning Part 2: Offline RL</td><td></td><td>Guest Lecture</td></tr><tr><td>11/12</td><td>RL for Robotics</td><td></td><td></td></tr><tr><td>11/12</td><td>HW-6 Due</td><td></td><td></td></tr><tr><td>11/19</td><td>Part 1: RL for protein folding Part 2: RL for circuit design</td><td></td><td>Guest Lecture</td></tr><tr><td>12/3</td><td>Current frontiers</td><td></td><td></td></tr><tr><td>12/10</td><td>Final Project Presentations and Writeups</td><td></td><td></td></tr></tbody></table></div></article><div class="margin-vert--xl"><div class="row"><div class="col"><a href="https://github.com/facebook/create-react-app/edit/master/docusaurus/website/../docs/lectures.md" target="_blank" rel="noreferrer noopener"><svg fill="currentColor" height="1.2em" width="1.2em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 40 40" style="margin-right:0.3em;vertical-align:sub"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col text--right"><em><small>Last updated on <time datetime="2021-06-20T22:40:43.000Z" class="docLastUpdatedAt_217_">6/20/2021</time> by <strong>lerrel</strong></small></em></div></div></div><div class="margin-vert--lg"><nav class="pagination-nav" aria-label="Blog list page navigation"><div class="pagination-nav__item"></div><div class="pagination-nav__item pagination-nav__item--next"></div></nav></div></div></div><div class="col col--3"><div class="tableOfContents_3SO_"></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container"><div class="row footer__links"><div class="col footer__col"><h4 class="footer__title">Resources</h4><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/docs/lectures">Schedule</a></li></ul></div><div class="col footer__col"><h4 class="footer__title">Community</h4><ul class="footer__items"><li class="footer__item"><a href="https://campuswire.com" target="_blank" rel="noopener noreferrer" class="footer__link-item">Class Campuswire</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/logistics#course-staff">Staff information</a></li></ul></div></div><div class="text--center"><div>Copyright Â© 2021 NYU Generalizable Robotics and AI Lab</div></div></div></footer></div>
<script src="/styles.bbcfc533.js"></script>
<script src="/runtime~main.f419612e.js"></script>
<script src="/main.99f6f5e6.js"></script>
<script src="/common.13e9cc2e.js"></script>
<script src="/13.3d134228.js"></script>
<script src="/16.0e408970.js"></script>
<script src="/935f2afb.3e930e98.js"></script>
<script src="/17896441.a64c684f.js"></script>
<script src="/883ca456.aa2bee24.js"></script>
</body>
</html>