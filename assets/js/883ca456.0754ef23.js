(self.webpackChunk=self.webpackChunk||[]).push([[254],{6506:function(t,e,a){"use strict";a.r(e),a.d(e,{frontMatter:function(){return p},contentTitle:function(){return d},metadata:function(){return m},toc:function(){return u},default:function(){return k}});var n=a(2122),r=a(9756),l=(a(7294),a(3905)),i=["components"],p={id:"lectures",title:"Course Schedule"},d=void 0,m={unversionedId:"lectures",id:"lectures",isDocsHomePage:!1,title:"Course Schedule",description:"| wk | Lecture | Links | Reading material |",source:"@site/../docs/lectures.md",sourceDirName:".",slug:"/lectures",permalink:"/deep-rl-class/docs/lectures",version:"current",lastUpdatedAt:1632588338,formattedLastUpdatedAt:"9/25/2021",frontMatter:{id:"lectures",title:"Course Schedule"}},u=[],o={toc:u};function k(t){var e=t.components,a=(0,r.Z)(t,i);return(0,l.kt)("wrapper",(0,n.Z)({},o,a,{components:e,mdxType:"MDXLayout"}),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null},"wk"),(0,l.kt)("th",{parentName:"tr",align:null},"Lecture"),(0,l.kt)("th",{parentName:"tr",align:null},"Links"),(0,l.kt)("th",{parentName:"tr",align:null},"Reading material"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"9/1"),(0,l.kt)("td",{parentName:"tr",align:null},"HW-0 Release"),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"td",href:"https://campuswire.com/c/G7204E992/feed/2"},"Campuswire")),(0,l.kt)("td",{parentName:"tr",align:null},"Fundamentals of Linear Algebra and Deep Learning")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"9/3"),(0,l.kt)("td",{parentName:"tr",align:null},"Part 1: Course overview ",(0,l.kt)("br",null),"Part 2: Supervised learning for control"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("br",null),"1","."," ",(0,l.kt)("a",{parentName:"td",href:"http://www.cse.unsw.edu.au/~claude/papers/MI15.pdf"},"A framework for behavioural cloning, Bain and Sommut, 1999."),(0,l.kt)("br",null),"2","."," ",(0,l.kt)("a",{parentName:"td",href:"http://proceedings.mlr.press/v15/ross11a/ross11a.pdf"},"A reduction of imitation learning and structured prediction to no-regret online learning, Ross et al., 2011."))),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"9/10"),(0,l.kt)("td",{parentName:"tr",align:null},"Part 1: Introduction to RL",(0,l.kt)("br",null),"Part 2: Value functions"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"Part 1: Chapter 1 of ",(0,l.kt)("a",{parentName:"td",href:"http://incompleteideas.net/book/RLbook2020.pdf"},"RL book"),".",(0,l.kt)("br",null),"Part 2: Chapter 3 of ",(0,l.kt)("a",{parentName:"td",href:"http://incompleteideas.net/book/RLbook2020.pdf"},"RL book"),".")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"9/10"),(0,l.kt)("td",{parentName:"tr",align:null},"HW-0 Due"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null})),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"9/10"),(0,l.kt)("td",{parentName:"tr",align:null},"HW-1 Release"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"Imitation via Supervision.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"9/17"),(0,l.kt)("td",{parentName:"tr",align:null},"Deep Q Learning"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"1","."," ",(0,l.kt)("a",{parentName:"td",href:"https://daiwk.github.io/assets/dqn.pdf"},"Human-level control through deep reinforcement learning, Mnih et al., 2015"),".",(0,l.kt)("br",null),"2","."," ",(0,l.kt)("a",{parentName:"td",href:"https://arxiv.org/pdf/1710.02298.pdf"},"Rainbow: Combining Improvements in Deep Reinforcement Learning, Hessel et al., 2017."),"\xa0",(0,l.kt)("br",null)," 3","."," ",(0,l.kt)("a",{parentName:"td",href:"https://deepmind.com/blog/article/Agent57-Outperforming-the-human-Atari-benchmark"},"Agent 57, Deepmind blog post"))),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"9/24"),(0,l.kt)("td",{parentName:"tr",align:null},"HW-1 Due"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null})),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"9/24"),(0,l.kt)("td",{parentName:"tr",align:null},"HW-2 Release"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"Deep Q Learning++.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"9/24"),(0,l.kt)("td",{parentName:"tr",align:null},"Policy gradients"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"1","."," ",(0,l.kt)("a",{parentName:"td",href:"https://link.springer.com/content/pdf/10.1007/BF00992696.pdf"},"REINFORCE, Williams, 1992."),(0,l.kt)("br",null),"2","."," ",(0,l.kt)("a",{parentName:"td",href:"https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf"},"A Natural Policy Gradient, Kakade, 2002"),".",(0,l.kt)("br",null),"3","."," ",(0,l.kt)("a",{parentName:"td",href:"https://arxiv.org/pdf/1707.06347.pdf"},"Proximal Policy Optimization Algorithms, Schulman et al. 2017."))),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"10/1"),(0,l.kt)("td",{parentName:"tr",align:null},"Part 1: Actor-critic methods",(0,l.kt)("br",null),"Part 2: Distributed RL"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"1","."," ",(0,l.kt)("a",{parentName:"td",href:"http://proceedings.mlr.press/v32/silver14.pdf"},"Deterministic Policy Gradient Algorithms, Silver et al., 2014."),(0,l.kt)("br",null),"2","."," ",(0,l.kt)("a",{parentName:"td",href:"https://arxiv.org/pdf/1602.01783.pdf"},"Asynchronous Methods for Deep Reinforcement Learning, Mnih et al. 2016."),(0,l.kt)("br",null),"3","."," ",(0,l.kt)("a",{parentName:"td",href:"https://arxiv.org/pdf/1507.04296.pdf"},"Massively Parallel Methods for Deep Reinforcement Learning, Nair et al. 2015."))),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"10/8"),(0,l.kt)("td",{parentName:"tr",align:null},"HW-2 Due"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null})),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"10/8"),(0,l.kt)("td",{parentName:"tr",align:null},"HW-3 Release"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"RL with Policy Gradients.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"10/8"),(0,l.kt)("td",{parentName:"tr",align:null},"Exploration in RL"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"1","."," ",(0,l.kt)("a",{parentName:"td",href:"https://lilianweng.github.io/lil-log/2020/06/07/exploration-strategies-in-deep-reinforcement-learning.html"},"Exploration strategies in deep RL, blog by Lilian Weng, 2020")," ",(0,l.kt)("br",null)," 2","."," ",(0,l.kt)("a",{parentName:"td",href:"http://www.pyoudeyer.com/ims.pdf"},"Intrinsic Motivation Systems for Autonomous Mental Development, Oudeyer et al. 2007.")," ",(0,l.kt)("br",null)," 3","."," ",(0,l.kt)("a",{parentName:"td",href:"https://arxiv.org/pdf/1705.05363.pdf"},"Curiosity-driven Exploration by Self-supervised Prediction, Pathak et al. 2017."))),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"10/9"),(0,l.kt)("td",{parentName:"tr",align:null},"Project Proposals Due"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null})),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"10/15"),(0,l.kt)("td",{parentName:"tr",align:null},"Generalization in RL"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"1","."," ",(0,l.kt)("a",{parentName:"td",href:"https://openai.com/blog/quantifying-generalization-in-reinforcement-learning"},"Quantifying Generalization in RL, blog by OpenAI 2018"),(0,l.kt)("br",null)," 2","."," ",(0,l.kt)("a",{parentName:"td",href:"https://arxiv.org/pdf/1509.06825.pdf"},"Supersizing Self-supervision: Learning to Grasp from 50K Tries and 700 Robot Hours, Pinto and Gupta 2016")," ",(0,l.kt)("br",null),"3","."," ",(0,l.kt)("a",{parentName:"td",href:"https://arxiv.org/pdf/2008.04899.pdf"},"Visual Imitation Made Easy, Young et al. 2020"))),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"10/15"),(0,l.kt)("td",{parentName:"tr",align:null},"HW-3 Due"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null})),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"10/15"),(0,l.kt)("td",{parentName:"tr",align:null},"HW-4 Release"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"Exploration with Bandits.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"10/22"),(0,l.kt)("td",{parentName:"tr",align:null},"Imitation Learning"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"1","."," ",(0,l.kt)("a",{parentName:"td",href:"https://ai.stanford.edu/~ang/papers/icml04-apprentice.pdf"},"Apprenticeship Learning via Inverse Reinforcement Learning, Abbeel and Ng 2004")," ",(0,l.kt)("br",null),"2","."," ",(0,l.kt)("a",{parentName:"td",href:"https://arxiv.org/pdf/1606.03476.pdf"},"Generative Adversarial Imitation Learning, Ho et al. 2016")," ",(0,l.kt)("br",null),"3","."," ",(0,l.kt)("a",{parentName:"td",href:"https://arxiv.org/pdf/1709.10087.pdf"},"Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations, Rajeswaran et al. 2018"))),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"10/22"),(0,l.kt)("td",{parentName:"tr",align:null},"HW-4 Due"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null})),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"10/22"),(0,l.kt)("td",{parentName:"tr",align:null},"HW-5 Release"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"Adaptive Control and Optimization.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"10/29"),(0,l.kt)("td",{parentName:"tr",align:null},"Control and planning"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"1","."," ",(0,l.kt)("a",{parentName:"td",href:"https://homes.cs.washington.edu/~todorov/papers/LiICINCO04.pdf"},"Iterative Linear Quadratic Regulator Design for Nonlinear Biological Movement Systems, Li and Todorov 2004")," ",(0,l.kt)("br",null),"2","."," ",(0,l.kt)("a",{parentName:"td",href:"https://arxiv.org/pdf/1907.02057.pdf"},"Benchmarking Model-Based Reinforcement Learning, Wang et al. 2019"))),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"10/29"),(0,l.kt)("td",{parentName:"tr",align:null},"HW-5 Due"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null})),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"10/29"),(0,l.kt)("td",{parentName:"tr",align:null},"HW-6 Release"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"Learning General Policies.")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"11/5"),(0,l.kt)("td",{parentName:"tr",align:null},"TBD -- ",(0,l.kt)("a",{parentName:"td",href:"https://www.microsoft.com/en-us/research/people/jcl/"},"John Langford")),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"Guest Lecture")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"11/12"),(0,l.kt)("td",{parentName:"tr",align:null},"Multi-Agent Learning -- ",(0,l.kt)("a",{parentName:"td",href:"https://www.cs.cmu.edu/~noamb/"},"Noam Brown")),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"Guest Lecture")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"11/12"),(0,l.kt)("td",{parentName:"tr",align:null},"HW-6 Due"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null})),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"11/19"),(0,l.kt)("td",{parentName:"tr",align:null},"Unsupervised Reinforcement Learning -- ",(0,l.kt)("a",{parentName:"td",href:"https://cs.nyu.edu/~dy1042/"},"Denis Yarats")),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null},"Guest Lecture")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"12/3"),(0,l.kt)("td",{parentName:"tr",align:null},"Current frontiers"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null})),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"12/10"),(0,l.kt)("td",{parentName:"tr",align:null},"Final Project Presentations and Writeups"),(0,l.kt)("td",{parentName:"tr",align:null}),(0,l.kt)("td",{parentName:"tr",align:null})))))}k.isMDXComponent=!0}}]);