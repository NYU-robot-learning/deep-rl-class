(window.webpackJsonp=window.webpackJsonp||[]).push([[6],{64:function(t,e,a){"use strict";a.r(e),a.d(e,"frontMatter",(function(){return l})),a.d(e,"metadata",(function(){return c})),a.d(e,"rightToc",(function(){return i})),a.d(e,"default",(function(){return j}));var n=a(2),b=a(6),r=(a(0),a(79)),l={id:"lectures",title:"Course Schedule"},c={unversionedId:"lectures",id:"lectures",isDocsHomePage:!1,title:"Course Schedule",description:"| wk | Lecture | Slides | Reading material |",source:"@site/../docs/lectures.md",slug:"/lectures",permalink:"/docs/lectures",editUrl:"https://github.com/facebook/create-react-app/edit/master/docusaurus/website/../docs/lectures.md",version:"current",lastUpdatedBy:"lerrel",lastUpdatedAt:1624228843},i=[],O={rightToc:i};function j(t){var e=t.components,a=Object(b.a)(t,["components"]);return Object(r.b)("wrapper",Object(n.a)({},O,a,{components:e,mdxType:"MDXLayout"}),Object(r.b)("table",null,Object(r.b)("thead",{parentName:"table"},Object(r.b)("tr",{parentName:"thead"},Object(r.b)("th",Object(n.a)({parentName:"tr"},{align:null}),"wk"),Object(r.b)("th",Object(n.a)({parentName:"tr"},{align:null}),"Lecture"),Object(r.b)("th",Object(n.a)({parentName:"tr"},{align:null}),"Slides"),Object(r.b)("th",Object(n.a)({parentName:"tr"},{align:null}),"Reading material"))),Object(r.b)("tbody",{parentName:"table"},Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"9/1"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"HW-0 Release"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null})),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"Fundamentals of Linear Algebra and Deep Learning")),Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"9/3"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"Part 1: Course overview ",Object(r.b)("br",null),"Part 2: Supervised learning for control"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null})),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),Object(r.b)("br",null),"1","."," ",Object(r.b)("a",Object(n.a)({parentName:"td"},{href:"http://www.cse.unsw.edu.au/~claude/papers/MI15.pdf"}),"A framework for behavioural cloning, Bain and Sommut, 1999."),Object(r.b)("br",null),"2","."," ",Object(r.b)("a",Object(n.a)({parentName:"td"},{href:"http://proceedings.mlr.press/v15/ross11a/ross11a.pdf"}),"A reduction of imitation learning and structured prediction to no-regret online learning, Ross et al., 2011."))),Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"9/10"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"Part 1: Introduction to RL",Object(r.b)("br",null),"Part 2: Value functions"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null})),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"Part 1: Chapter 1 of ",Object(r.b)("a",Object(n.a)({parentName:"td"},{href:"http://incompleteideas.net/book/RLbook2020.pdf"}),"RL book"),".",Object(r.b)("br",null),"Part 2: Chapter 3 of ",Object(r.b)("a",Object(n.a)({parentName:"td"},{href:"http://incompleteideas.net/book/RLbook2020.pdf"}),"RL book"),".")),Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"9/10"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"HW-0 Due"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null})),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}))),Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"9/10"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"HW-1 Release"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null})),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"Imitation via Supervision.")),Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"9/17"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"Deep Q Learning"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null})),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"1","."," ",Object(r.b)("a",Object(n.a)({parentName:"td"},{href:"https://daiwk.github.io/assets/dqn.pdf"}),"Human-level control through deep reinforcement learning, Mnih et al., 2015"),".",Object(r.b)("br",null),"2","."," ",Object(r.b)("a",Object(n.a)({parentName:"td"},{href:"https://arxiv.org/pdf/1710.02298.pdf"}),"Rainbow: Combining Improvements in Deep Reinforcement Learning, Hessel et al., 2017."),"\xa0",Object(r.b)("br",null)," 3","."," ",Object(r.b)("a",Object(n.a)({parentName:"td"},{href:"https://deepmind.com/blog/article/Agent57-Outperforming-the-human-Atari-benchmark"}),"Agent 57, Deepmind blog post"))),Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"9/17"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"HW-1 Due"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null})),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}))),Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"9/17"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"HW-2 Release"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null})),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"Deep Q Learning++.")),Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"9/24"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"Policy gradients"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null})),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"1","."," ",Object(r.b)("a",Object(n.a)({parentName:"td"},{href:"https://link.springer.com/content/pdf/10.1007/BF00992696.pdf"}),"REINFORCE, Williams, 1992."),Object(r.b)("br",null),"2","."," ",Object(r.b)("a",Object(n.a)({parentName:"td"},{href:"https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf"}),"A Natural Policy Gradient, Kakade, 2002"),".",Object(r.b)("br",null),"3","."," ",Object(r.b)("a",Object(n.a)({parentName:"td"},{href:"https://arxiv.org/pdf/1707.06347.pdf"}),"Proximal Policy Optimization Algorithms, Schulman et al. 2017."))),Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"10/1"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"Part 1: Actor-critic methods",Object(r.b)("br",null),"Part 2: Distributed RL"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null})),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"1","."," ",Object(r.b)("a",Object(n.a)({parentName:"td"},{href:"http://proceedings.mlr.press/v32/silver14.pdf"}),"Deterministic Policy Gradient Algorithms, Silver et al., 2014."),Object(r.b)("br",null),"2","."," ",Object(r.b)("a",Object(n.a)({parentName:"td"},{href:"https://arxiv.org/pdf/1602.01783.pdf"}),"Asynchronous Methods for Deep Reinforcement Learning, Mnih et al. 2016."),Object(r.b)("br",null),"3","."," ",Object(r.b)("a",Object(n.a)({parentName:"td"},{href:"https://arxiv.org/pdf/1507.04296.pdf"}),"Massively Parallel Methods for Deep Reinforcement Learning, Nair et al. 2015."))),Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"10/1"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"HW-2 Due"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null})),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}))),Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"10/1"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"HW-3 Release"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null})),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"RL with Policy Gradients.")),Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"10/8"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"Exploration in RL"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null})),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"1","."," ",Object(r.b)("a",Object(n.a)({parentName:"td"},{href:"https://lilianweng.github.io/lil-log/2020/06/07/exploration-strategies-in-deep-reinforcement-learning.html"}),"Exploration strategies in deep RL, blog by Lilian Weng, 2020")," ",Object(r.b)("br",null)," 2","."," ",Object(r.b)("a",Object(n.a)({parentName:"td"},{href:"http://www.pyoudeyer.com/ims.pdf"}),"Intrinsic Motivation Systems for Autonomous Mental Development, Oudeyer et al. 2007.")," ",Object(r.b)("br",null)," 3","."," ",Object(r.b)("a",Object(n.a)({parentName:"td"},{href:"https://arxiv.org/pdf/1705.05363.pdf"}),"Curiosity-driven Exploration by Self-supervised Prediction, Pathak et al. 2017."))),Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"10/9"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"Project Proposals Due"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null})),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}))),Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"10/15"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"Generalization in RL"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null})),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"1","."," ",Object(r.b)("a",Object(n.a)({parentName:"td"},{href:"https://openai.com/blog/quantifying-generalization-in-reinforcement-learning"}),"Quantifying Generalization in RL, blog by OpenAI 2018"),Object(r.b)("br",null)," 2","."," ",Object(r.b)("a",Object(n.a)({parentName:"td"},{href:"https://arxiv.org/pdf/1509.06825.pdf"}),"Supersizing Self-supervision: Learning to Grasp from 50K Tries and 700 Robot Hours, Pinto and Gupta 2016")," ",Object(r.b)("br",null),"3","."," ",Object(r.b)("a",Object(n.a)({parentName:"td"},{href:"https://arxiv.org/pdf/2008.04899.pdf"}),"Visual Imitation Made Easy, Young et al. 2020"))),Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"10/15"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"HW-3 Due"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null})),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}))),Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"10/15"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"HW-4 Release"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null})),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"Exploration with Bandits.")),Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"10/22"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"Imitation Learning"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null})),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"1","."," ",Object(r.b)("a",Object(n.a)({parentName:"td"},{href:"https://ai.stanford.edu/~ang/papers/icml04-apprentice.pdf"}),"Apprenticeship Learning via Inverse Reinforcement Learning, Abbeel and Ng 2004")," ",Object(r.b)("br",null),"2","."," ",Object(r.b)("a",Object(n.a)({parentName:"td"},{href:"https://arxiv.org/pdf/1606.03476.pdf"}),"Generative Adversarial Imitation Learning, Ho et al. 2016")," ",Object(r.b)("br",null),"3","."," ",Object(r.b)("a",Object(n.a)({parentName:"td"},{href:"https://arxiv.org/pdf/1709.10087.pdf"}),"Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations, Rajeswaran et al. 2018"))),Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"10/22"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"HW-4 Due"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null})),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}))),Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"10/22"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"HW-5 Release"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null})),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"Adaptive Control and Optimization.")),Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"10/29"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"Control and planning"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null})),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"1","."," ",Object(r.b)("a",Object(n.a)({parentName:"td"},{href:"https://homes.cs.washington.edu/~todorov/papers/LiICINCO04.pdf"}),"Iterative Linear Quadratic Regulator Design for Nonlinear Biological Movement Systems, Li and Todorov 2004")," ",Object(r.b)("br",null),"2","."," ",Object(r.b)("a",Object(n.a)({parentName:"td"},{href:"https://arxiv.org/pdf/1907.02057.pdf"}),"Benchmarking Model-Based Reinforcement Learning, Wang et al. 2019"))),Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"10/29"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"HW-5 Due"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null})),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}))),Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"10/29"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"HW-6 Release"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null})),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"Learning General Policies.")),Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"11/5"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"Part 1: Meta learning Part 2: Offline RL"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null})),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"Guest Lecture")),Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"11/12"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"RL for Robotics"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null})),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}))),Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"11/12"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"HW-6 Due"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null})),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}))),Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"11/19"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"Part 1: RL for protein folding Part 2: RL for circuit design"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null})),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"Guest Lecture")),Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"12/3"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"Current frontiers"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null})),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}))),Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"12/10"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"Final Project Presentations and Writeups"),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null})),Object(r.b)("td",Object(n.a)({parentName:"tr"},{align:null}))))))}j.isMDXComponent=!0}}]);