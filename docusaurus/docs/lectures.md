---
id: lectures
title: Lecture Schedule
---
 
| wk | Lecture | Link | Reading material |
|---|---|---|---|
| 9/4 | Part 1: Course overview <br />Part 2: Supervised learning for control | | Part 2<br />1\. [A framework for behavioural cloning, Bain and Sommut, 1999.](http://www.cse.unsw.edu.au/~claude/papers/MI15.pdf)<br />2\. [A reduction of imitation learning and structured prediction to no-regret online learning, Ross et al., 2011.](http://proceedings.mlr.press/v15/ross11a/ross11a.pdf) |
| 9/11 | Part 1: Introduction to RL<br />Part 2: Value functions | |Part 1: Chapter 1 of [RL book](http://incompleteideas.net/book/RLbook2020.pdf).<br />Part 2: Chapter 3 of [RL book](http://incompleteideas.net/book/RLbook2020.pdf). |
| 9/18 | Deep Q Learning | |1\. [Human-level control through deep reinforcement learning, Mnih et al., 2015](https://daiwk.github.io/assets/dqn.pdf).<br />2\. [Rainbow: Combining Improvements in Deep Reinforcement Learning, Hessel et al., 2017.](https://arxiv.org/pdf/1710.02298.pdf)Â <br /> 3\. [Agent 57, Deepmind blog post](https://deepmind.com/blog/article/Agent57-Outperforming-the-human-Atari-benchmark) |
| 9/25 | Policy gradients | |1\. [REINFORCE, Williams, 1992.](https://link.springer.com/content/pdf/10.1007/BF00992696.pdf)<br />2\. [A Natural Policy Gradient, Kakade, 2002](https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf).<br />3\. [Proximal Policy Optimization Algorithms, Schulman et al. 2017.](https://arxiv.org/pdf/1707.06347.pdf) |
| 10/2 | Part 1: Actor-critic methods<br />Part 2: Distributed RL | | 1\. [Deterministic Policy Gradient Algorithms, Silver et al., 2014.](http://proceedings.mlr.press/v32/silver14.pdf)<br />2\. [Asynchronous Methods for Deep Reinforcement Learning, Mnih et al. 2016.](https://arxiv.org/pdf/1602.01783.pdf)<br />3\. [Massively Parallel Methods for Deep Reinforcement Learning, Nair et al. 2015.](https://arxiv.org/pdf/1507.04296.pdf) |
| 10/9  | Exploration in RL  |  | 1\. [Exploration strategies in deep RL, blog by Lilian Weng](https://lilianweng.github.io/lil-log/2020/06/07/exploration-strategies-in-deep-reinforcement-learning.html) 2020 2\. [Intrinsic Motivation Systems for Autonomous Mental Development, Oudeyer et al. 2007.](http://www.pyoudeyer.com/ims.pdf) 3\. [Curiosity-driven Exploration by Self-supervised Prediction, Pathak et al. 2017.](https://arxiv.org/pdf/1705.05363.pdf)  |
| 10/16  | Generalization in RL  |  | 1\. [Quantifying Generalization in RL, blog by OpenAI 2018](https://openai.com/blog/quantifying-generalization-in-reinforcement-learning) 2\. [Supersizing Self-supervision: Learning to Grasp from 50K Tries and 700 Robot Hours, Pinto and Gupta 2016](https://arxiv.org/pdf/1509.06825.pdf) 3\. [Visual Imitation Made Easy, Young et al. 2020](https://arxiv.org/pdf/2008.04899.pdf)  |
| 10/23  | Imitation Learning  |  | 1\. [Apprenticeship Learning via Inverse Reinforcement Learning, Abbeel and Ng 2004](https://ai.stanford.edu/~ang/papers/icml04-apprentice.pdf) 2\. [Generative Adversarial Imitation Learning, Ho et al. 2016](https://arxiv.org/pdf/1606.03476.pdf) 3\. [Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations, Rajeswaran et al. 2018](https://arxiv.org/pdf/1709.10087.pdf)  |
| 10/30  | Control and planning  |  | 1\. [Iterative Linear Quadratic Regulator Design for Nonlinear Biological Movement Systems, Li and Todorov 2004](https://homes.cs.washington.edu/~todorov/papers/LiICINCO04.pdf) 2\. [Benchmarking Model-Based Reinforcement Learning, Wang et al. 2019](https://arxiv.org/pdf/1907.02057.pdf)  |
| 11/6  | Part 1: Meta learning Part 2: Offline RL  |  | External talk by [Abhishek Gupta](https://people.eecs.berkeley.edu/~abhigupta/)  |
| 11/13  | RL for Robotics  |  |  |
| 11/20  | Part 1: RL for protein folding Part 2: RL for circuit design  |  |  |
| 12/4  | Current frontiers  |  |  |
| 12/11  | Final Project Presentations  |  |  |